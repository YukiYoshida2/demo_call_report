#!/usr/bin/env python3
"""
ãƒ‡ãƒ¢é›»è©±ãƒãƒ¼ãƒ  æœˆæ¬¡é€²æ—åˆ†æ â€” ç¢ºå®šè¨ˆç®—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å…¨åˆ†æãƒ†ãƒ¼ãƒ–ãƒ«ã‚’Pythonã§è¨ˆç®—ã—ã€Markdownãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦ data/computed/ ã«å‡ºåŠ›ã™ã‚‹ã€‚
LLMã«ã‚ˆã‚‹è¨ˆç®—ã‚’æ’é™¤ã—ã€å†ç¾å¯èƒ½ã§æ­£ç¢ºãªæ•°å€¤ã‚’ä¿è¨¼ã™ã‚‹ã€‚

Usage:
    python3 scripts/compute_tables.py --date 2026-02-25
"""

import argparse
import csv
import sys
from collections import defaultdict
from datetime import datetime, timedelta
from pathlib import Path

# ================================================================
# å®šæ•°
# ================================================================

IS_REPS = ["æ¹¯æœ¬ éš†å—£", "æ°¸é‡ é›ª", "æ‘æ¾ äºœèŒ‰éŸ³", "ä¸­æœ¬ é™½ä»‹"]
OUTSOURCE_REPS = ["ä¸­é‡Œ å¥å¤ª", "é‡‘ æ—­å…‰"]
CHANNEL_ORDER = ["å…¨ä½“", "TOP", "LIS", "DIS", "FAXãƒ»EDM", "ãã®ä»–"]
CHANNELS = ["TOP", "LIS", "DIS", "FAXãƒ»EDM", "ãã®ä»–"]

PP_WORSEN = -5.0
BELOW_AVG_RATIO = 0.20
IMPACT_WARN = -3
IMPACT_CRIT = -5
WEEKLY_DROP = -15.0
CROSS_CHANNEL_WARN = -10.0

CSV_PREFIXES = {
    "q1": "ç€åœ°äºˆæƒ³",
    "q2": "SALç€äºˆ",
    "q3": "å•†è«‡å®Ÿæ–½ç€äºˆ",
    "q4": "ãƒ‡ãƒ¢é›»è©±",
    "q5": "SALç‡_ç©ã¿ä¸Šã’",
    "q6": "ãƒ‡ãƒ¢é›»è©±_å•†è«‡",
}

Q4_REQUIRED = [
    "id", "reasons_for_ineligible_leads", "inflow_route_media",
    "cv_content_sub__c", "is_connect", "is_sal", "is_task_complete",
    "created_date_jst", "month", "business_hours_class", "is_holiday",
    "phone_type_flag", "user_name",
]

Q5_REQUIRED = [
    "created_date_jst", "demo_call_type_summary_v2", "cv_content_sub__c",
    "total_leads", "total_sal", "sal_within_1d", "sal_within_3d",
    "sal_7d_diff", "sal_14d_diff", "sal_21d_diff", "sal_30d_diff",
    "sal_after_30d",
]


# ================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ================================================================

def safe_div(num, den):
    if den == 0:
        return None
    return num / den


def fmt_pct(val, dec=1):
    if val is None:
        return "-"
    return f"{val * 100:.{dec}f}%"


def fmt_pp(val, warn_threshold=PP_WORSEN):
    if val is None:
        return "N/A"
    sign = "+" if val >= 0 else ""
    mark = "ğŸ“‰" if val <= warn_threshold else ""
    return f"{sign}{val:.1f}pp{mark}"


def fmt_count_diff(cur, prev):
    if prev is None or prev == 0:
        return "N/A"
    diff_pct = (cur - prev) / prev * 100
    sign = "+" if diff_pct >= 0 else ""
    return f"{sign}{diff_pct:.1f}%"


def fmt_int(val):
    if val is None:
        return "-"
    return f"{val:,}"


def prev_month_str(ym):
    """'2026-02' â†’ '2026-01'"""
    y, m = int(ym[:4]), int(ym[5:7])
    if m == 1:
        return f"{y - 1}-12"
    return f"{y}-{m - 1:02d}"


def parse_date(s):
    if not s:
        return None
    try:
        return datetime.strptime(s[:10], "%Y-%m-%d").date()
    except (ValueError, IndexError):
        return None


def iso_week_label(d):
    iso = d.isocalendar()
    monday = d - timedelta(days=d.weekday())
    return f"W{iso[1]:02d} ({monday.month}/{monday.day}-)"


def iso_week_key(d):
    iso = d.isocalendar()
    return (iso[0], iso[1])


def md_table(headers, rows):
    lines = []
    lines.append("| " + " | ".join(headers) + " |")
    lines.append("|" + "|".join(["---"] * len(headers)) + "|")
    for row in rows:
        lines.append("| " + " | ".join(str(c) for c in row) + " |")
    return "\n".join(lines)


def frontmatter(data_date, current_month, previous_month,
                 period_start="", period_end=""):
    now = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
    return f"""---
computed_at: {now}
data_date: {data_date}
current_month: {current_month}
previous_month: {previous_month}
period_start: {period_start}
period_end: {period_end}
---

"""


def write_file(path, content):
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)


# ================================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ================================================================

def find_csv(data_dir, query_id, date_str):
    prefix = CSV_PREFIXES[query_id]
    expected = f"{prefix}-{date_str}.csv"
    path = data_dir / expected
    return path if path.exists() else None


def find_prev_month_csv(data_dir, query_id, current_date_str):
    prefix = CSV_PREFIXES[query_id]
    current = datetime.strptime(current_date_str, "%Y-%m-%d").date()
    if current.month == 1:
        prev_y, prev_m = current.year - 1, 12
    else:
        prev_y, prev_m = current.year, current.month - 1

    candidates = []
    for f in sorted(data_dir.iterdir()):
        if not f.name.startswith(prefix + "-"):
            continue
        date_part = f.name[len(prefix) + 1 : -4]  # "ç€åœ°äºˆæƒ³-2026-01-25.csv" â†’ "2026-01-25"
        try:
            file_date = datetime.strptime(date_part, "%Y-%m-%d").date()
            if file_date.year == prev_y and file_date.month == prev_m:
                candidates.append(f)
        except ValueError:
            pass
    return candidates[-1] if candidates else None


def load_csv_file(filepath):
    rows = []
    with open(filepath, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)
    return rows


# ================================================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ•ã‚£ãƒ«ã‚¿
# ================================================================

def is_eligible(row):
    val = row.get("reasons_for_ineligible_leads", "")
    return val == "" or val.lower() == "null"


def get_row_month(row):
    m = row.get("month", "")
    if m and m != "" and m.lower() != "null":
        return m[:7]
    dt = row.get("created_date_jst", "")
    if dt:
        return dt[:7]
    return None


def detect_current_month_q4(rows):
    months = set()
    for row in rows:
        if is_eligible(row):
            m = get_row_month(row)
            if m:
                months.add(m)
    return sorted(months)[-1] if months else None


def detect_current_month_q5(rows):
    months = set()
    for row in rows:
        dt = row.get("created_date_jst", "")
        if dt and len(dt) >= 7:
            months.add(dt[:7])
    return sorted(months)[-1] if months else None


def filter_q4(rows, month_str):
    return [r for r in rows if is_eligible(r) and get_row_month(r) == month_str]


def filter_q5(rows, month_str):
    return [r for r in rows if r.get("created_date_jst", "")[:7] == month_str]


# ================================================================
# Q1-Q3 å‰æœˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå‰æœˆCSVãŒãªã„å ´åˆã€Q4/Q6ã‹ã‚‰ä»£æ›¿è¨ˆç®—ï¼‰
# ================================================================

def build_prev_actuals_from_q4(q4_prev):
    """Q4å‰æœˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„ãƒãƒ£ãƒãƒ«ã®ç€é›»æ•°ã‚’é›†è¨ˆã€‚Q1å‰æœˆCSVã®ä»£æ›¿ã€‚"""
    ch_ids = defaultdict(set)
    for r in q4_prev:
        ch = r.get("inflow_route_media", "ãã®ä»–")
        rid = r.get("id", "")
        if rid:
            ch_ids[ch].add(rid)
    result = {}
    total = 0
    for ch in CHANNELS:
        c = len(ch_ids.get(ch, set()))
        result[ch] = c
        total += c
    result["å…¨ä½“"] = total
    return result


def build_prev_sal_from_q4(q4_prev):
    """Q4å‰æœˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„ãƒãƒ£ãƒãƒ«ã®SALæ•°ã‚’é›†è¨ˆã€‚Q2å‰æœˆCSVã®ä»£æ›¿ã€‚"""
    ch_ids = defaultdict(set)
    for r in q4_prev:
        if str(r.get("is_sal", "0")) == "1":
            ch = r.get("inflow_route_media", "ãã®ä»–")
            rid = r.get("id", "")
            if rid:
                ch_ids[ch].add(rid)
    result = {}
    total = 0
    for ch in CHANNELS:
        c = len(ch_ids.get(ch, set()))
        result[ch] = c
        total += c
    result["å…¨ä½“"] = total
    return result


def build_prev_meetings_from_q6(q6, prev_month):
    """Q6ã‹ã‚‰å‰æœˆã®å•†è«‡å®Ÿæ–½æ•°ã‚’ãƒãƒ£ãƒãƒ«åˆ¥ã«é›†è¨ˆã€‚Q3å‰æœˆCSVã®ä»£æ›¿ã€‚"""
    valid_channels = {"TOP", "LIS", "DIS", "FAXãƒ»EDM"}
    ch_count = defaultdict(int)
    for r in q6:
        fmt = r.get("first_meeting_date", "")
        if not fmt or fmt[:7] != prev_month:
            continue
        ch = r.get("inflow_route_media_lasttouch", "")
        if ch not in valid_channels:
            ch = "ãã®ä»–"
        ch_count[ch] += 1
    result = {}
    total = 0
    for ch in CHANNELS:
        c = ch_count.get(ch, 0)
        result[ch] = c
        total += c
    result["å…¨ä½“"] = total
    return result


def extract_targets(q_rows):
    """Q1-Q3 CSVã‹ã‚‰ãƒãƒ£ãƒãƒ«åˆ¥æœˆé–“ç›®æ¨™ã‚’æŠ½å‡ºã€‚"""
    targets = {}
    for row in q_rows:
        dim = row.get("dimension", "")
        t = row.get("monthly_target", "")
        if dim and t:
            targets[dim] = int(t)
    return targets


# ================================================================
# é›†è¨ˆãƒ˜ãƒ«ãƒ‘ãƒ¼
# ================================================================

def count_distinct(rows, key="id"):
    return len(set(r[key] for r in rows if r.get(key)))


def count_distinct_where(rows, predicate, key="id"):
    return len(set(r[key] for r in rows if predicate(r) and r.get(key)))


def compute_funnel(rows):
    leads = count_distinct(rows)
    connects = count_distinct_where(
        rows, lambda r: str(r.get("is_connect", "0")) == "1"
    )
    sals = count_distinct_where(
        rows, lambda r: str(r.get("is_sal", "0")) == "1"
    )
    tasks = count_distinct_where(
        rows, lambda r: r.get("is_task_complete", "") == "å®Œäº†"
    )
    return {
        "leads": leads,
        "connects": connects,
        "sals": sals,
        "tasks": tasks,
        "cn_rate": safe_div(connects, leads),
        "sal_rate": safe_div(sals, connects),
        "task_rate": safe_div(tasks, leads),
    }


def group_by(rows, key_func):
    groups = defaultdict(list)
    for row in rows:
        k = key_func(row)
        if k is not None:
            groups[k].append(row)
    return dict(groups)


def classify_user(user_name):
    if user_name in IS_REPS:
        return user_name
    if user_name in OUTSOURCE_REPS:
        return "å¤–æ³¨ï¼ˆåˆç®—ï¼‰"
    return None


def pp_diff(cur_rate, prev_rate):
    if cur_rate is not None and prev_rate is not None:
        return (cur_rate - prev_rate) * 100
    return None


# ================================================================
# STEP 1: æ•°å€¤é€²æ—ã‚µãƒãƒª
# ================================================================

def compute_step1_landing(q_rows, prev_q_rows, label,
                          fallback_prev_actuals=None):
    if not q_rows:
        return f"â€» {label}ã®CSVãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“", {}

    latest = {}
    for row in q_rows:
        dim = row["dimension"]
        ld = row["lead_date"]
        if dim not in latest or ld > latest[dim]["lead_date"]:
            latest[dim] = row

    prev_latest = {}
    if prev_q_rows:
        # å¾“æ¥é€šã‚Š: å‰æœˆCSVã‹ã‚‰å‰æœˆé”æˆç‡ã‚’å–å¾—
        for row in prev_q_rows:
            dim = row["dimension"]
            ld = row["lead_date"]
            if dim not in prev_latest or ld > prev_latest[dim]["lead_date"]:
                prev_latest[dim] = row
    elif fallback_prev_actuals:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Q4/Q6ã‹ã‚‰ä»£æ›¿è¨ˆç®—ã—ãŸå‰æœˆç¢ºå®šå®Ÿç¸¾ã‚’ä½¿ç”¨
        targets = extract_targets(q_rows)
        for ch, actual in fallback_prev_actuals.items():
            target = targets.get(ch, 0)
            ach = safe_div(actual, target) if target > 0 else None
            prev_latest[ch] = {"achievement_pct": ach, "lead_date": "fallback"}

    headers = [
        "ãƒãƒ£ãƒãƒ«", "å®Ÿç¸¾ç´¯è¨ˆ", "ç€åœ°äºˆæ¸¬", "æœˆç›®æ¨™", "é”æˆç‡",
        "å‰æœˆé”æˆç‡", "å‰æœˆæ¯”", "åˆ¤å®š",
    ]
    rows_out = []
    results = {}

    for ch in CHANNEL_ORDER:
        if ch not in latest:
            continue
        r = latest[ch]
        cum = int(r["cumulative_actual"]) if r["cumulative_actual"] else 0
        forecast_raw = r.get("landing_forecast", "")
        forecast = int(float(forecast_raw)) if forecast_raw else None
        target = int(r["monthly_target"]) if r["monthly_target"] else 0
        ach_raw = r.get("achievement_pct", "")
        ach = float(ach_raw) if ach_raw else None

        prev_ach = None
        if ch in prev_latest:
            pa = prev_latest[ch].get("achievement_pct", "")
            if isinstance(pa, (int, float)):
                prev_ach = pa
            elif pa:
                prev_ach = float(pa)

        if ach is not None:
            judgment = "âœ…" if ach >= 1.0 else "âŒ"
            ach_display = f"{ach * 100:.0f}%"
        else:
            judgment = "-"
            ach_display = "-"

        prev_display = f"{prev_ach * 100:.0f}%" if prev_ach is not None else "N/A"

        if ach is not None and prev_ach is not None:
            diff_val = (ach - prev_ach) * 100
            diff_display = fmt_pp(diff_val)
        else:
            diff_display = "N/A"

        ch_display = f"**{ch}**" if ch == "å…¨ä½“" else ch
        rows_out.append([
            ch_display, fmt_int(cum),
            fmt_int(forecast) if forecast is not None else "-",
            fmt_int(target), ach_display, prev_display, diff_display, judgment,
        ])
        results[ch] = {"ach": ach, "judgment": judgment, "prev_ach": prev_ach}

    table = md_table(headers, rows_out)
    if not prev_q_rows and not fallback_prev_actuals:
        table += "\n\nâ€» å‰æœˆCSVãŒãƒ•ã‚©ãƒ«ãƒ€ã«ãªã„ãŸã‚ã€å‰æœˆé”æˆç‡ãƒ»å‰æœˆæ¯”ã¯N/A"
    elif fallback_prev_actuals:
        table += "\n\nâ€» å‰æœˆé”æˆç‡ã¯Q4/Q6å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ä»£æ›¿è¨ˆç®—å€¤"

    return table, results


def compute_step1_issues(results_1, results_2, results_3):
    headers = ["ãƒãƒ£ãƒãƒ«", "ç€é›»", "SAL", "å•†è«‡å®Ÿæ–½", "Badæ•°", "ä½ç½®ã¥ã‘"]
    rows_out = []

    for ch in CHANNELS:
        r1 = results_1.get(ch, {})
        r2 = results_2.get(ch, {})
        r3 = results_3.get(ch, {})

        j1 = r1.get("judgment", "-")
        j2 = r2.get("judgment", "-")
        j3 = r3.get("judgment", "-")
        a1 = r1.get("ach")
        a2 = r2.get("ach")
        a3 = r3.get("ach")

        ach1 = f"{j1} {a1 * 100:.0f}%" if a1 is not None else "-"
        ach2 = f"{j2} {a2 * 100:.0f}%" if a2 is not None else "-"
        ach3 = f"{j3} {a3 * 100:.0f}%" if a3 is not None else "-"

        bad_count = sum(1 for j in [j1, j2, j3] if j == "âŒ")

        if bad_count >= 2:
            position = "**é‡ç‚¹èª²é¡Œ**"
        elif bad_count == 1:
            position = "èª²é¡Œ"
        else:
            position = "-"

        rows_out.append([
            f"**{ch}**" if bad_count >= 2 else ch,
            ach1, ach2, ach3, str(bad_count), position,
        ])

    rows_out.sort(key=lambda r: (-int(r[4]), r[0]))

    issue_channels = [r[0].replace("**", "") for r in rows_out if int(r[4]) > 0]

    table = md_table(headers, rows_out)

    # Add commentary for issue channels
    commentary = []
    for ch in CHANNELS:
        r1 = results_1.get(ch, {})
        r2 = results_2.get(ch, {})
        r3 = results_3.get(ch, {})
        bads = []
        if r1.get("judgment") == "âŒ":
            bads.append(f"ç€é›»{r1['ach'] * 100:.0f}%")
        if r2.get("judgment") == "âŒ":
            bads.append(f"SAL{r2['ach'] * 100:.0f}%")
        if r3.get("judgment") == "âŒ":
            bads.append(f"å•†è«‡{r3['ach'] * 100:.0f}%")
        if bads:
            commentary.append(f"- **{ch}**: {', '.join(bads)}")

    if commentary:
        table += "\n\n" + "\n".join(commentary)

    return table


# ================================================================
# STEP 2-1: ãƒ•ã‚¡ãƒãƒ«è»¢æ›ç‡
# ================================================================

def compute_step2_funnel(q4_cur, q4_prev):
    cur_groups = group_by(q4_cur, lambda r: r.get("inflow_route_media", ""))
    prev_groups = group_by(q4_prev, lambda r: r.get("inflow_route_media", ""))

    headers = [
        "ãƒãƒ£ãƒãƒ«", "ãƒªãƒ¼ãƒ‰æ•°", "å‰æœˆæ¯”", "CNç‡", "å‰æœˆæ¯”",
        "SALç‡", "å‰æœˆæ¯”", "ã‚¿ã‚¹ã‚¯å®Œäº†ç‡", "å‰æœˆæ¯”",
    ]
    rows_out = []
    cur_metrics = {}
    prev_metrics = {}

    for ch in CHANNELS:
        cm = compute_funnel(cur_groups.get(ch, []))
        pm = compute_funnel(prev_groups.get(ch, []))
        cur_metrics[ch] = cm
        prev_metrics[ch] = pm

        rows_out.append([
            ch,
            fmt_int(cm["leads"]),
            fmt_count_diff(cm["leads"], pm["leads"]),
            fmt_pct(cm["cn_rate"]),
            fmt_pp(pp_diff(cm["cn_rate"], pm["cn_rate"])),
            fmt_pct(cm["sal_rate"]),
            fmt_pp(pp_diff(cm["sal_rate"], pm["sal_rate"])),
            fmt_pct(cm["task_rate"]),
            fmt_pp(pp_diff(cm["task_rate"], pm["task_rate"])),
        ])

    main_table = md_table(headers, rows_out)

    # Reference: absolute numbers
    ref_headers = [
        "ãƒãƒ£ãƒãƒ«", "å½“æœˆãƒªãƒ¼ãƒ‰", "å½“æœˆCN", "å½“æœˆSAL",
        "å‰æœˆãƒªãƒ¼ãƒ‰", "å‰æœˆCN", "å‰æœˆSAL",
    ]
    ref_rows = []
    for ch in CHANNELS:
        cm = cur_metrics[ch]
        pm = prev_metrics[ch]
        ref_rows.append([
            ch,
            fmt_int(cm["leads"]), fmt_int(cm["connects"]), fmt_int(cm["sals"]),
            fmt_int(pm["leads"]), fmt_int(pm["connects"]), fmt_int(pm["sals"]),
        ])

    ref_table = md_table(ref_headers, ref_rows)

    return main_table + "\n\nå‚è€ƒ: çµ¶å¯¾æ•°\n\n" + ref_table, cur_metrics, prev_metrics


# ================================================================
# STEP 2-2: CVã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ¥
# ================================================================

def compute_step2_cv(q4_cur, q4_prev, cur_channel_metrics):
    output_sections = []

    for ch in CHANNELS:
        ch_rows_cur = [r for r in q4_cur if r.get("inflow_route_media") == ch]
        ch_rows_prev = [r for r in q4_prev if r.get("inflow_route_media") == ch]

        if not ch_rows_cur:
            continue

        ch_avg = cur_channel_metrics.get(ch, {})
        ch_cn_avg = ch_avg.get("cn_rate")
        ch_sal_avg = ch_avg.get("sal_rate")

        cv_groups_cur = group_by(
            ch_rows_cur,
            lambda r: r.get("cv_content_sub__c") or "(ç©º)",
        )
        cv_groups_prev = group_by(
            ch_rows_prev,
            lambda r: r.get("cv_content_sub__c") or "(ç©º)",
        )

        cv_data = []
        for cv, rows in cv_groups_cur.items():
            m = compute_funnel(rows)
            pm = compute_funnel(cv_groups_prev.get(cv, []))
            cv_data.append((cv, m, pm))

        cv_data.sort(key=lambda x: -x[1]["leads"])
        top10 = cv_data[:10]

        headers = [
            "CVã‚³ãƒ³ãƒ†ãƒ³ãƒ„", "ãƒªãƒ¼ãƒ‰æ•°", "CNç‡", "å·®åˆ†",
            "SALç‡", "å·®åˆ†", "å‰æœˆCNæ¯”", "å‰æœˆSALæ¯”",
        ]
        rows_out = []

        for cv, m, pm in top10:
            # vs channel average
            cn_vs_avg = pp_diff(m["cn_rate"], ch_cn_avg)
            sal_vs_avg = pp_diff(m["sal_rate"], ch_sal_avg)

            cn_flag = ""
            if (ch_cn_avg and m["cn_rate"] is not None
                    and m["cn_rate"] < ch_cn_avg * (1 - BELOW_AVG_RATIO)):
                cn_flag = "âš ï¸"
            sal_flag = ""
            if (ch_sal_avg and m["sal_rate"] is not None
                    and m["sal_rate"] < ch_sal_avg * (1 - BELOW_AVG_RATIO)):
                sal_flag = "âš ï¸"

            cn_diff_s = (
                f"{cn_vs_avg:+.1f}pp{cn_flag}"
                if cn_vs_avg is not None else "-"
            )
            sal_diff_s = (
                f"{sal_vs_avg:+.1f}pp{sal_flag}"
                if sal_vs_avg is not None else "-"
            )

            # vs previous month
            if pm["leads"] == 0:
                cn_prev_s = "æ–°è¦"
                sal_prev_s = "æ–°è¦"
            else:
                cn_prev_val = pp_diff(m["cn_rate"], pm["cn_rate"])
                sal_prev_val = pp_diff(m["sal_rate"], pm["sal_rate"])
                cn_prev_s = fmt_pp(cn_prev_val)
                sal_prev_s = fmt_pp(sal_prev_val)

            rows_out.append([
                cv, fmt_int(m["leads"]),
                fmt_pct(m["cn_rate"]), cn_diff_s,
                fmt_pct(m["sal_rate"]), sal_diff_s,
                cn_prev_s, sal_prev_s,
            ])

        ch_section = f"#### {ch} Top10 CVã‚³ãƒ³ãƒ†ãƒ³ãƒ„\n\n"
        ch_section += (
            f"ãƒãƒ£ãƒãƒ«å¹³å‡: CNç‡={fmt_pct(ch_cn_avg)}, "
            f"SALç‡={fmt_pct(ch_sal_avg)}\n\n"
        )
        ch_section += md_table(headers, rows_out)
        output_sections.append(ch_section)

    return "\n\n".join(output_sections)


# ================================================================
# STEP 2-3: SALã‚¹ãƒ”ãƒ¼ãƒ‰åˆ†æ
# ================================================================

def compute_step2_sal_speed(q5_cur, q5_prev):
    def aggregate(rows):
        groups = group_by(rows, lambda r: r.get("demo_call_type_summary_v2", ""))
        result = {}
        for ch, ch_rows in groups.items():
            if not ch:
                continue
            tl = sum(int(r.get("total_leads", 0)) for r in ch_rows)
            ts = sum(int(r.get("total_sal", 0)) for r in ch_rows)
            w1 = sum(int(r.get("sal_within_1d", 0)) for r in ch_rows)
            w3 = sum(int(r.get("sal_within_3d", 0)) for r in ch_rows)
            d7 = sum(int(r.get("sal_7d_diff", 0)) for r in ch_rows)
            d14 = sum(int(r.get("sal_14d_diff", 0)) for r in ch_rows)
            d21 = sum(int(r.get("sal_21d_diff", 0)) for r in ch_rows)
            d30 = sum(int(r.get("sal_30d_diff", 0)) for r in ch_rows)

            c3 = w1 + w3
            c7 = c3 + d7
            c14 = c7 + d14
            c21 = c14 + d21
            c30 = c21 + d30

            result[ch] = {
                "total_leads": tl, "total_sal": ts,
                "w1d_rate": safe_div(w1, tl),
                "cum_3d_rate": safe_div(c3, tl),
                "cum_7d_rate": safe_div(c7, tl),
                "cum_14d_rate": safe_div(c14, tl),
                "cum_30d_rate": safe_div(c30, tl),
            }
        return result

    cur = aggregate(q5_cur)
    prev = aggregate(q5_prev)

    headers = [
        "ãƒãƒ£ãƒãƒ«", "ãƒªãƒ¼ãƒ‰æ•°", "SALæ•°", "1æ—¥ä»¥å†…", "å‰æœˆæ¯”",
        "3æ—¥ä»¥å†…", "å‰æœˆæ¯”", "7æ—¥ä»¥å†…", "å‰æœˆæ¯”", "14æ—¥ä»¥å†…", "30æ—¥ä»¥å†…",
    ]
    rows_out = []

    all_channels = sorted(set(list(cur.keys()) + list(prev.keys())))
    for ch in all_channels:
        cm = cur.get(ch, {})
        pm = prev.get(ch, {})

        rows_out.append([
            ch,
            fmt_int(cm.get("total_leads", 0)),
            fmt_int(cm.get("total_sal", 0)),
            fmt_pct(cm.get("w1d_rate")),
            fmt_pp(pp_diff(cm.get("w1d_rate"), pm.get("w1d_rate"))),
            fmt_pct(cm.get("cum_3d_rate")),
            fmt_pp(pp_diff(cm.get("cum_3d_rate"), pm.get("cum_3d_rate"))),
            fmt_pct(cm.get("cum_7d_rate")),
            fmt_pp(pp_diff(cm.get("cum_7d_rate"), pm.get("cum_7d_rate"))),
            fmt_pct(cm.get("cum_14d_rate")),
            fmt_pct(cm.get("cum_30d_rate")),
        ])

    return md_table(headers, rows_out)


# ================================================================
# STEP 2-4: æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰
# ================================================================

def compute_step2_timeseries(q4_cur):
    sections = []

    # Weekly trend per channel
    for ch in CHANNELS:
        ch_rows = [r for r in q4_cur if r.get("inflow_route_media") == ch]
        if not ch_rows:
            continue

        weekly_groups = defaultdict(list)
        for row in ch_rows:
            d = parse_date(row.get("created_date_jst", ""))
            if d:
                weekly_groups[iso_week_key(d)].append(row)

        wk_headers = ["é€±", "ãƒªãƒ¼ãƒ‰æ•°", "CNç‡", "SALç‡"]
        wk_rows = []
        for wk in sorted(weekly_groups.keys()):
            rows = weekly_groups[wk]
            m = compute_funnel(rows)
            dates = [
                parse_date(r.get("created_date_jst", ""))
                for r in rows
                if parse_date(r.get("created_date_jst", ""))
            ]
            label = iso_week_label(min(dates)) if dates else f"W{wk[1]:02d}"
            wk_rows.append([
                label, fmt_int(m["leads"]),
                fmt_pct(m["cn_rate"]), fmt_pct(m["sal_rate"]),
            ])

        if wk_rows:
            sections.append(
                f"#### {ch} é€±åˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰\n\n" + md_table(wk_headers, wk_rows)
            )

    # Business hours comparison
    bh_headers = ["åŒºåˆ†", "ãƒãƒ£ãƒãƒ«", "ãƒªãƒ¼ãƒ‰æ•°", "CNç‡", "SALç‡"]
    bh_rows = []
    for ch in CHANNELS:
        ch_rows = [r for r in q4_cur if r.get("inflow_route_media") == ch]
        bh_groups = group_by(ch_rows, lambda r: r.get("business_hours_class", ""))
        for bh in ["å–¶æ¥­æ™‚é–“å†…(10_19)", "å–¶æ¥­æ™‚é–“å¤–"]:
            bh_r = bh_groups.get(bh, [])
            if bh_r:
                m = compute_funnel(bh_r)
                lbl = "å–¶æ¥­æ™‚é–“å†…" if "å†…" in bh else "å–¶æ¥­æ™‚é–“å¤–"
                bh_rows.append([
                    lbl, ch, fmt_int(m["leads"]),
                    fmt_pct(m["cn_rate"]), fmt_pct(m["sal_rate"]),
                ])

    if bh_rows:
        sections.append(
            "#### å–¶æ¥­æ™‚é–“å¸¯åˆ¥æ¯”è¼ƒ\n\n" + md_table(bh_headers, bh_rows)
        )

    # Holiday comparison
    hol_headers = ["åŒºåˆ†", "ãƒãƒ£ãƒãƒ«", "ãƒªãƒ¼ãƒ‰æ•°", "CNç‡", "SALç‡"]
    hol_rows = []
    for ch in CHANNELS:
        ch_rows = [r for r in q4_cur if r.get("inflow_route_media") == ch]
        hol_groups = group_by(ch_rows, lambda r: r.get("is_holiday", ""))
        for hol in ["å¹³æ—¥", "ä¼‘æ—¥"]:
            hol_r = hol_groups.get(hol, [])
            if hol_r:
                m = compute_funnel(hol_r)
                hol_rows.append([
                    hol, ch, fmt_int(m["leads"]),
                    fmt_pct(m["cn_rate"]), fmt_pct(m["sal_rate"]),
                ])

    if hol_rows:
        sections.append(
            "#### å¹³æ—¥/ä¼‘æ—¥æ¯”è¼ƒ\n\n" + md_table(hol_headers, hol_rows)
        )

    return "\n\n".join(sections)


# ================================================================
# STEP 2-5: æ‹…å½“è€…åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
# ================================================================

def filter_analysis_reps(rows):
    result = []
    for row in rows:
        cls = classify_user(row.get("user_name", ""))
        if cls is not None:
            row_copy = dict(row)
            row_copy["_rep"] = cls
            result.append(row_copy)
    return result


def compute_step2_user_summary(q4_cur, q4_prev):
    cur_reps = filter_analysis_reps(q4_cur)
    prev_reps = filter_analysis_reps(q4_prev)

    overall_cur = compute_funnel(cur_reps)
    overall_prev = compute_funnel(prev_reps)

    cur_groups = group_by(cur_reps, lambda r: r["_rep"])
    prev_groups = group_by(prev_reps, lambda r: r["_rep"])

    headers = [
        "æ‹…å½“è€…", "ãƒªãƒ¼ãƒ‰æ•°", "CNç‡", "vså¹³å‡", "vså‰æœˆ",
        "SALç‡", "vså¹³å‡", "vså‰æœˆ", "ã‚¿ã‚¹ã‚¯å®Œäº†ç‡", "vså‰æœˆ", "è¦æ³¨æ„",
    ]
    rows_out = []

    # Overall average row
    rows_out.append([
        "**å…¨ä½“å¹³å‡**",
        fmt_int(overall_cur["leads"]),
        fmt_pct(overall_cur["cn_rate"]),
        "-",
        fmt_pp(pp_diff(overall_cur["cn_rate"], overall_prev["cn_rate"])),
        fmt_pct(overall_cur["sal_rate"]),
        "-",
        fmt_pp(pp_diff(overall_cur["sal_rate"], overall_prev["sal_rate"])),
        fmt_pct(overall_cur["task_rate"]),
        fmt_pp(pp_diff(overall_cur["task_rate"], overall_prev["task_rate"])),
        "-",
    ])

    rep_order = IS_REPS + ["å¤–æ³¨ï¼ˆåˆç®—ï¼‰"]
    for rep in rep_order:
        cm = compute_funnel(cur_groups.get(rep, []))
        pm = compute_funnel(prev_groups.get(rep, []))

        cn_va = pp_diff(cm["cn_rate"], overall_cur["cn_rate"])
        sal_va = pp_diff(cm["sal_rate"], overall_cur["sal_rate"])
        cn_vp = pp_diff(cm["cn_rate"], pm["cn_rate"])
        sal_vp = pp_diff(cm["sal_rate"], pm["sal_rate"])
        task_vp = pp_diff(cm["task_rate"], pm["task_rate"])

        warnings = []
        if (overall_cur["cn_rate"] and cm["cn_rate"] is not None
                and cm["cn_rate"] < overall_cur["cn_rate"] * (1 - BELOW_AVG_RATIO)):
            warnings.append("âš ï¸CNç‡")
        if (overall_cur["sal_rate"] and cm["sal_rate"] is not None
                and cm["sal_rate"] < overall_cur["sal_rate"] * (1 - BELOW_AVG_RATIO)):
            warnings.append("âš ï¸SALç‡")
        if cn_vp is not None and cn_vp <= PP_WORSEN:
            warnings.append("ğŸ“‰CN")
        if sal_vp is not None and sal_vp <= PP_WORSEN:
            warnings.append("ğŸ“‰SAL")

        rep_display = f"**{rep}**" if rep == "å¤–æ³¨ï¼ˆåˆç®—ï¼‰" else rep
        rows_out.append([
            rep_display,
            fmt_int(cm["leads"]),
            fmt_pct(cm["cn_rate"]),
            fmt_pp(cn_va, warn_threshold=-999),
            fmt_pp(cn_vp),
            fmt_pct(cm["sal_rate"]),
            fmt_pp(sal_va, warn_threshold=-999),
            fmt_pp(sal_vp),
            fmt_pct(cm["task_rate"]),
            fmt_pp(task_vp),
            ", ".join(warnings) if warnings else "-",
        ])

    return md_table(headers, rows_out)


def compute_step2_user_channel(q4_cur):
    cur_reps = filter_analysis_reps(q4_cur)

    ch_groups = group_by(cur_reps, lambda r: r.get("inflow_route_media", ""))
    ch_avgs = {ch: compute_funnel(rows) for ch, rows in ch_groups.items()}

    headers = [
        "æ‹…å½“è€…", "ãƒãƒ£ãƒãƒ«", "ãƒªãƒ¼ãƒ‰æ•°", "CNç‡", "å·®åˆ†",
        "SALç‡", "å·®åˆ†", "è¦æ³¨æ„",
    ]
    rows_out = []

    rep_order = IS_REPS + ["å¤–æ³¨ï¼ˆåˆç®—ï¼‰"]
    for rep in rep_order:
        rep_rows = [r for r in cur_reps if r["_rep"] == rep]
        rep_ch = group_by(rep_rows, lambda r: r.get("inflow_route_media", ""))

        for ch in CHANNELS:
            ch_rows = rep_ch.get(ch, [])
            if not ch_rows:
                continue

            m = compute_funnel(ch_rows)
            avg = ch_avgs.get(ch, {})

            cn_d = pp_diff(m["cn_rate"], avg.get("cn_rate"))
            sal_d = pp_diff(m["sal_rate"], avg.get("sal_rate"))

            warns = []
            if cn_d is not None and cn_d <= CROSS_CHANNEL_WARN:
                warns.append("âš ï¸CNç‡")
            if sal_d is not None and sal_d <= CROSS_CHANNEL_WARN:
                warns.append("âš ï¸SALç‡")

            rows_out.append([
                rep, ch, fmt_int(m["leads"]),
                fmt_pct(m["cn_rate"]),
                fmt_pp(cn_d, warn_threshold=-999),
                fmt_pct(m["sal_rate"]),
                fmt_pp(sal_d, warn_threshold=-999),
                ", ".join(warns) if warns else "-",
            ])

    return md_table(headers, rows_out)


def compute_step2_user_impact(q4_cur):
    cur_reps = filter_analysis_reps(q4_cur)

    # Channel SAL/leads rate (overall, from analysis reps)
    ch_groups = group_by(cur_reps, lambda r: r.get("inflow_route_media", ""))
    ch_sal_rates = {}
    for ch, rows in ch_groups.items():
        m = compute_funnel(rows)
        ch_sal_rates[ch] = safe_div(m["sals"], m["leads"])

    headers = [
        "ãƒãƒ£ãƒãƒ«", "æ‹…å½“è€…", "ãƒªãƒ¼ãƒ‰æ•°", "å®ŸSAL", "æœŸå¾…SAL", "å·®åˆ†", "åˆ¤å®š",
    ]
    rows_out = []

    rep_order = IS_REPS + ["å¤–æ³¨ï¼ˆåˆç®—ï¼‰"]
    for ch in CHANNELS:
        ch_reps_rows = [r for r in cur_reps if r.get("inflow_route_media") == ch]
        if not ch_reps_rows:
            continue

        ch_rate = ch_sal_rates.get(ch)
        if ch_rate is None:
            continue

        rep_groups = group_by(ch_reps_rows, lambda r: r["_rep"])

        for rep in rep_order:
            rep_rows = rep_groups.get(rep, [])
            if not rep_rows:
                continue

            leads = count_distinct(rep_rows)
            sals = count_distinct_where(
                rep_rows, lambda r: str(r.get("is_sal", "0")) == "1"
            )
            expected = leads * ch_rate
            diff = sals - expected

            if diff <= IMPACT_CRIT:
                judgment = "ğŸš¨"
            elif diff <= IMPACT_WARN:
                judgment = "âš ï¸"
            else:
                judgment = "-"

            diff_s = f"{diff:+.1f}"
            if judgment != "-":
                diff_s = f"**{diff_s}**"

            rows_out.append([
                ch, rep, fmt_int(leads), fmt_int(sals),
                f"{expected:.1f}", diff_s, judgment,
            ])

    return md_table(headers, rows_out)


def compute_step2_user_weekly(q4_cur):
    cur_reps = filter_analysis_reps(q4_cur)
    alerts = []

    rep_order = IS_REPS + ["å¤–æ³¨ï¼ˆåˆç®—ï¼‰"]
    for rep in rep_order:
        rep_rows = [r for r in cur_reps if r["_rep"] == rep]

        weekly = defaultdict(list)
        for row in rep_rows:
            d = parse_date(row.get("created_date_jst", ""))
            if d:
                weekly[iso_week_key(d)].append(row)

        prev_cn = None
        prev_sal = None

        for wk in sorted(weekly.keys()):
            rows = weekly[wk]
            m = compute_funnel(rows)

            dates = [
                parse_date(r.get("created_date_jst", ""))
                for r in rows
                if parse_date(r.get("created_date_jst", ""))
            ]
            label = iso_week_label(min(dates)) if dates else f"W{wk[1]:02d}"

            if prev_cn is not None and m["cn_rate"] is not None:
                cn_d = (m["cn_rate"] - prev_cn) * 100
                if cn_d <= WEEKLY_DROP:
                    alerts.append({
                        "rep": rep, "week": label, "metric": "CNç‡",
                        "value": fmt_pct(m["cn_rate"]),
                        "diff": f"{cn_d:+.1f}pp",
                        "leads": fmt_int(m["leads"]),
                    })

            if prev_sal is not None and m["sal_rate"] is not None:
                sal_d = (m["sal_rate"] - prev_sal) * 100
                if sal_d <= WEEKLY_DROP:
                    alerts.append({
                        "rep": rep, "week": label, "metric": "SALç‡",
                        "value": fmt_pct(m["sal_rate"]),
                        "diff": f"{sal_d:+.1f}pp",
                        "leads": fmt_int(m["leads"]),
                    })

            prev_cn = m["cn_rate"]
            prev_sal = m["sal_rate"]

    if not alerts:
        return "æ€¥è½ã¯æ¤œçŸ¥ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"

    headers = ["æ‹…å½“è€…", "é€±", "æŒ‡æ¨™", "å€¤", "å‰é€±æ¯”", "ãƒªãƒ¼ãƒ‰æ•°"]
    rows_out = [
        [a["rep"], a["week"], a["metric"], a["value"], a["diff"], a["leads"]]
        for a in alerts
    ]
    return md_table(headers, rows_out)


# ================================================================
# ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
# ================================================================

def validate_data(data_dir, date_str, q1, q2, q3, q4, q5, q6):
    lines = ["# ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ\n"]
    warnings = []
    errors = []

    lines.append("## ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§\n")
    files_info = [
        ("Q1 ç€åœ°äºˆæƒ³", q1), ("Q2 SALç€äºˆ", q2), ("Q3 å•†è«‡å®Ÿæ–½ç€äºˆ", q3),
        ("Q4 ãƒ‡ãƒ¢é›»è©±", q4), ("Q5 SALç‡_ç©ã¿ä¸Šã’", q5), ("Q6 ãƒ‡ãƒ¢é›»è©±_å•†è«‡", q6),
    ]
    lines.append("| ã‚¯ã‚¨ãƒª | è¡Œæ•° | ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ |")
    lines.append("|--------|------|----------|")
    for name, rows in files_info:
        if rows is not None:
            lines.append(f"| {name} | {len(rows):,} | OK |")
        else:
            lines.append(f"| {name} | - | ãƒ•ã‚¡ã‚¤ãƒ«ãªã— |")
            if "Q4" in name or "Q1" in name or "Q2" in name or "Q3" in name:
                errors.append(f"{name}: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # Column checks
    if q4:
        missing = [c for c in Q4_REQUIRED if c not in q4[0]]
        if missing:
            errors.append(f"Q4 å¿…é ˆã‚«ãƒ©ãƒ ä¸è¶³: {missing}")
        else:
            lines.append("\n- Q4 å¿…é ˆã‚«ãƒ©ãƒ : å…¨ã¦å­˜åœ¨ âœ“")

    if q5:
        missing = [c for c in Q5_REQUIRED if c not in q5[0]]
        if missing:
            errors.append(f"Q5 å¿…é ˆã‚«ãƒ©ãƒ ä¸è¶³: {missing}")
        else:
            lines.append("- Q5 å¿…é ˆã‚«ãƒ©ãƒ : å…¨ã¦å­˜åœ¨ âœ“")

    # Previous day row count comparison
    prev_date = (
        datetime.strptime(date_str, "%Y-%m-%d") - timedelta(days=1)
    ).strftime("%Y-%m-%d")
    prev_q4_path = find_csv(data_dir, "q4", prev_date)
    if prev_q4_path and q4:
        prev_q4 = load_csv_file(prev_q4_path)
        if prev_q4:
            ratio = len(q4) / len(prev_q4)
            if ratio < 0.8 or ratio > 1.2:
                warnings.append(
                    f"Q4 è¡Œæ•°å¤‰å‹•: å‰æ—¥{len(prev_q4):,}è¡Œ â†’ "
                    f"å½“æ—¥{len(q4):,}è¡Œ ({ratio:.1%})"
                )

    if errors:
        lines.append("\n## ã‚¨ãƒ©ãƒ¼\n")
        for e in errors:
            lines.append(f"- âŒ {e}")
    if warnings:
        lines.append("\n## è­¦å‘Š\n")
        for w in warnings:
            lines.append(f"- âš ï¸ {w}")
    if not errors and not warnings:
        lines.append("\n## çµæœ: å…¨ãƒã‚§ãƒƒã‚¯é€šé âœ…\n")

    return "\n".join(lines), len(errors) > 0


# ================================================================
# ãƒ¡ã‚¤ãƒ³
# ================================================================

def main():
    parser = argparse.ArgumentParser(
        description="ãƒ‡ãƒ¢é›»è©±ãƒãƒ¼ãƒ  æœˆæ¬¡åˆ†æãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºå®šè¨ˆç®—"
    )
    parser.add_argument("--date", required=True, help="ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜ (YYYY-MM-DD)")
    parser.add_argument("--data-dir", default="data", help="ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--output-dir", default="data/computed", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    args = parser.parse_args()

    data_dir = Path(args.data_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    date_str = args.date

    # ---- Load CSVs ----
    print(f"[1/7] CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­... (date={date_str})")

    q1 = q2 = q3 = q4 = q5 = q6 = None
    for qid in ["q1", "q2", "q3", "q4", "q5", "q6"]:
        path = find_csv(data_dir, qid, date_str)
        if path:
            data = load_csv_file(path)
            print(f"   {qid}: {path.name} ({len(data):,}è¡Œ)")
            if qid == "q1": q1 = data
            elif qid == "q2": q2 = data
            elif qid == "q3": q3 = data
            elif qid == "q4": q4 = data
            elif qid == "q5": q5 = data
            elif qid == "q6": q6 = data
        else:
            print(f"   {qid}: ãƒ•ã‚¡ã‚¤ãƒ«ãªã—")

    # ---- Validate ----
    print("[2/7] ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ä¸­...")
    validation_report, has_errors = validate_data(
        data_dir, date_str, q1, q2, q3, q4, q5, q6
    )
    write_file(output_dir / "_validation.md", validation_report)

    if has_errors:
        print(
            f"âŒ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ã€‚{output_dir}/_validation.md ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )
        sys.exit(1)

    # ---- Detect months ----
    current_month = detect_current_month_q4(q4) if q4 else None
    if not current_month:
        print("âŒ å½“æœˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    previous_month = prev_month_str(current_month)
    print(f"   å½“æœˆ: {current_month}, å‰æœˆ: {previous_month}")

    # ---- Filter Q4 ----
    q4_cur = filter_q4(q4, current_month) if q4 else []
    q4_prev = filter_q4(q4, previous_month) if q4 else []
    print(f"   Q4 eligible: å½“æœˆ={len(q4_cur):,}è¡Œ, å‰æœˆ={len(q4_prev):,}è¡Œ")

    # ---- Detect period ----
    cur_dates = sorted(set(
        r.get("created_date_jst", "")[:10]
        for r in q4_cur if r.get("created_date_jst")
    ))
    period_start = cur_dates[0] if cur_dates else ""
    period_end = cur_dates[-1] if cur_dates else ""
    if period_start:
        print(f"   å‚ç…§æœŸé–“: {period_start} ã€œ {period_end}")

    fm = frontmatter(date_str, current_month, previous_month,
                     period_start, period_end)

    # Previous month Q1-Q3 CSVs
    prev_q1_path = find_prev_month_csv(data_dir, "q1", date_str)
    prev_q2_path = find_prev_month_csv(data_dir, "q2", date_str)
    prev_q3_path = find_prev_month_csv(data_dir, "q3", date_str)
    prev_q1 = load_csv_file(prev_q1_path) if prev_q1_path else None
    prev_q2 = load_csv_file(prev_q2_path) if prev_q2_path else None
    prev_q3 = load_csv_file(prev_q3_path) if prev_q3_path else None

    # Fallback: Q4/Q6ã‹ã‚‰å‰æœˆå®Ÿç¸¾ã‚’æ§‹ç¯‰ï¼ˆå‰æœˆCSVãŒãªã„å ´åˆï¼‰
    fallback_q1 = fallback_q2 = fallback_q3 = None
    if not prev_q1 and q4_prev:
        fallback_q1 = build_prev_actuals_from_q4(q4_prev)
        print(f"   Q1å‰æœˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Q4ã‹ã‚‰ç€é›»æ•°ä»£æ›¿è¨ˆç®—")
    if not prev_q2 and q4_prev:
        fallback_q2 = build_prev_sal_from_q4(q4_prev)
        print(f"   Q2å‰æœˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Q4ã‹ã‚‰SALæ•°ä»£æ›¿è¨ˆç®—")
    if not prev_q3 and q6:
        fallback_q3 = build_prev_meetings_from_q6(q6, previous_month)
        print(f"   Q3å‰æœˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Q6ã‹ã‚‰å•†è«‡å®Ÿæ–½æ•°ä»£æ›¿è¨ˆç®—")

    # ---- STEP 1 ----
    print("[3/7] STEP1 è¨ˆç®—ä¸­...")
    table_1_1, results_1 = compute_step1_landing(q1, prev_q1, "ç€é›»",
                                                  fallback_q1)
    table_1_2, results_2 = compute_step1_landing(q2, prev_q2, "SAL",
                                                  fallback_q2)
    table_1_3, results_3 = compute_step1_landing(q3, prev_q3, "å•†è«‡å®Ÿæ–½",
                                                  fallback_q3)
    table_1_4 = compute_step1_issues(results_1, results_2, results_3)

    write_file(output_dir / "step1_ç€é›»ç€äºˆ.md", fm + table_1_1)
    write_file(output_dir / "step1_SALç€äºˆ.md", fm + table_1_2)
    write_file(output_dir / "step1_å•†è«‡å®Ÿæ–½ç€äºˆ.md", fm + table_1_3)
    write_file(output_dir / "step1_èª²é¡Œãƒãƒ£ãƒãƒ«.md", fm + table_1_4)

    # ---- STEP 2 ----
    print("[4/7] STEP2 ãƒ•ã‚¡ãƒãƒ«ãƒ»CVè¨ˆç®—ä¸­...")
    funnel_table, cur_ch, prev_ch = compute_step2_funnel(q4_cur, q4_prev)
    cv_table = compute_step2_cv(q4_cur, q4_prev, cur_ch)

    write_file(output_dir / "step2_ãƒ•ã‚¡ãƒãƒ«è»¢æ›ç‡.md", fm + funnel_table)
    write_file(output_dir / "step2_CVã‚³ãƒ³ãƒ†ãƒ³ãƒ„.md", fm + cv_table)

    print("[5/7] STEP2 SALã‚¹ãƒ”ãƒ¼ãƒ‰ãƒ»æ™‚ç³»åˆ—è¨ˆç®—ä¸­...")
    q5_cur = filter_q5(q5, current_month) if q5 else []
    q5_prev = filter_q5(q5, previous_month) if q5 else []
    print(f"   Q5: å½“æœˆ={len(q5_cur):,}è¡Œ, å‰æœˆ={len(q5_prev):,}è¡Œ")

    sal_speed_table = compute_step2_sal_speed(q5_cur, q5_prev)
    timeseries_table = compute_step2_timeseries(q4_cur)

    write_file(output_dir / "step2_SALã‚¹ãƒ”ãƒ¼ãƒ‰.md", fm + sal_speed_table)
    write_file(output_dir / "step2_æ™‚ç³»åˆ—.md", fm + timeseries_table)

    print("[6/7] STEP2 æ‹…å½“è€…åˆ†æè¨ˆç®—ä¸­...")
    user_summary = compute_step2_user_summary(q4_cur, q4_prev)
    user_channel = compute_step2_user_channel(q4_cur)
    user_impact = compute_step2_user_impact(q4_cur)
    user_weekly = compute_step2_user_weekly(q4_cur)

    write_file(output_dir / "step2_æ‹…å½“è€…ã‚µãƒãƒª.md", fm + user_summary)
    write_file(output_dir / "step2_æ‹…å½“è€…ãƒãƒ£ãƒãƒ«.md", fm + user_channel)
    write_file(output_dir / "step2_ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆè©¦ç®—.md", fm + user_impact)
    write_file(output_dir / "step2_é€±æ¬¡æ€¥è½.md", fm + user_weekly)

    # ---- Summary ----
    print("[7/7] å®Œäº†!")
    print(f"   å‡ºåŠ›å…ˆ: {output_dir}/")
    print(f"   ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 13")
    total_leads = count_distinct(q4_cur)
    print(f"   å½“æœˆeligible ãƒªãƒ¼ãƒ‰æ•°: {total_leads:,} ({current_month})")


if __name__ == "__main__":
    main()
