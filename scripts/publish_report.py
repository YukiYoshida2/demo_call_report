#!/usr/bin/env python3
"""
CIç”¨ãƒ¬ãƒãƒ¼ãƒˆå…¬é–‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ â€” NotionæŠ•ç¨¿ + Slacké€šçŸ¥

ãƒªã‚¹ã‚¯æœ€å°åŒ–è¨­è¨ˆ:
  - Notion: Internal Integrationï¼ˆå¯¾è±¡DBã®ã¿å…±æœ‰ã€æŒ¿å…¥æ¨©é™ã®ã¿ï¼‰
  - Slack: Incoming Webhook æ¨å¥¨ï¼ˆãƒãƒ£ãƒãƒ«é™å®šã€èª­å–ä¸å¯ï¼‰
  - æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ä½¿ç”¨ï¼ˆã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ãƒªã‚¹ã‚¯ãªã—ï¼‰

å¿…è¦ãªç’°å¢ƒå¤‰æ•°:
  NOTION_API_KEY     â€” Notion Internal Integration ãƒˆãƒ¼ã‚¯ãƒ³
  SLACK_WEBHOOK_URL  â€” Slack Incoming Webhook URLï¼ˆæ¨å¥¨ï¼‰

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
  NOTION_DATABASE_ID â€” Notion DB IDï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å¯¾è±¡DBï¼‰
  SLACK_BOT_TOKEN    â€” Slack Bot Tokenï¼ˆWebhookæœªè¨­å®šæ™‚ã®DMé€ä¿¡ç”¨ï¼‰
  SLACK_CHANNEL      â€” Slack ãƒãƒ£ãƒãƒ«/DM IDï¼ˆBot Tokenä½¿ç”¨æ™‚ï¼‰
  SLACK_MENTION_USER â€” ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³å…ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ID
"""

import json
import os
import re
import sys
from glob import glob
from urllib.request import Request, urlopen
from urllib.error import HTTPError, URLError
from datetime import datetime, timezone, timedelta
from pathlib import Path

# ================================================================
# å®šæ•°
# ================================================================

JST = timezone(timedelta(hours=9))
NOTION_API = "https://api.notion.com/v1"
NOTION_VER = "2022-06-28"
MAX_BLOCKS = 100
MAX_RT_LEN = 2000

DEFAULT_DB_ID = "311eea80-adae-80a5-a798-000bc1a1a73f"
DEFAULT_MENTIONS = ["U07EJ6YKUPK", "U05V0RAF09M", "U07LNE4G2R0"]
DEFAULT_CHANNEL = "C08PMM3C601"

CHANNEL_ORDER = ["å…¨ä½“", "TOP", "LIS", "DIS", "FAXãƒ»EDM", "ãã®ä»–"]


# ================================================================
# Computed table ãƒ‘ãƒ¼ã‚µãƒ¼
# ================================================================

def strip_md_bold(text):
    """Remove **bold** markers from text"""
    return re.sub(r'\*\*(.+?)\*\*', r'\1', text)


def parse_frontmatter(filepath):
    """Extract frontmatter key-value pairs from a computed markdown file."""
    with open(filepath, encoding="utf-8") as f:
        content = f.read()
    lines = content.split("\n")
    meta = {}
    in_fm = False
    for line in lines:
        if line.strip() == "---":
            if in_fm:
                break
            in_fm = True
            continue
        if in_fm and ":" in line:
            key, _, val = line.partition(":")
            meta[key.strip()] = val.strip()
    return meta


def parse_computed_table(filepath):
    """Parse a computed markdown table file (with frontmatter) into list of dicts."""
    with open(filepath, encoding="utf-8") as f:
        content = f.read()

    lines = content.split("\n")

    # Skip frontmatter (between first and second ---)
    fm_count = 0
    start = 0
    for i, line in enumerate(lines):
        if line.strip() == "---":
            fm_count += 1
            if fm_count == 2:
                start = i + 1
                break

    # Find and parse table rows
    table_rows = []
    for line in lines[start:]:
        stripped = line.strip()
        if not (stripped.startswith("|") and stripped.endswith("|")):
            if table_rows:
                break
            continue
        if re.match(r'^\s*\|[-:\s|]+\|\s*$', line):
            continue
        cells = [c.strip() for c in stripped.split("|")]
        if cells and cells[0] == "":
            cells = cells[1:]
        if cells and cells[-1] == "":
            cells = cells[:-1]
        table_rows.append(cells)

    if len(table_rows) < 2:
        return []

    # Handle duplicate headers (e.g. multiple "å‰æœˆæ¯”" columns)
    raw_headers = table_rows[0]
    seen = {}
    headers = []
    for h in raw_headers:
        if h in seen:
            seen[h] += 1
            headers.append(f"{h}_{seen[h]}")
        else:
            seen[h] = 0
            headers.append(h)

    result = []
    for row in table_rows[1:]:
        d = {}
        for j, h in enumerate(headers):
            d[h] = strip_md_bold(row[j]) if j < len(row) else ""
        result.append(d)
    return result


def parse_cv_tables(filepath):
    """Parse step2_CVã‚³ãƒ³ãƒ†ãƒ³ãƒ„.md (multi-section) into {channel: [row_dicts]}."""
    with open(filepath, encoding="utf-8") as f:
        content = f.read()

    lines = content.split("\n")

    # Skip frontmatter
    fm_count = 0
    start = 0
    for i, line in enumerate(lines):
        if line.strip() == "---":
            fm_count += 1
            if fm_count == 2:
                start = i + 1
                break

    result = {}
    current_channel = None
    table_lines = []

    def _flush():
        if not current_channel or not table_lines:
            return
        # Parse accumulated table lines as a mini table
        rows = []
        for tl in table_lines:
            stripped = tl.strip()
            if re.match(r'^\s*\|[-:\s|]+\|\s*$', tl):
                continue
            cells = [c.strip() for c in stripped.split("|")]
            if cells and cells[0] == "":
                cells = cells[1:]
            if cells and cells[-1] == "":
                cells = cells[:-1]
            rows.append(cells)
        if len(rows) >= 2:
            hdrs = rows[0]
            seen = {}
            unique_hdrs = []
            for h in hdrs:
                if h in seen:
                    seen[h] += 1
                    unique_hdrs.append(f"{h}_{seen[h]}")
                else:
                    seen[h] = 0
                    unique_hdrs.append(h)
            parsed = []
            for row in rows[1:]:
                d = {}
                for j, h in enumerate(unique_hdrs):
                    d[h] = strip_md_bold(row[j]) if j < len(row) else ""
                parsed.append(d)
            result[current_channel] = parsed

    for line in lines[start:]:
        m = re.match(r'^####\s+(\S+)\s+Top10', line)
        if m:
            _flush()
            current_channel = m.group(1)
            table_lines = []
            continue
        stripped = line.strip()
        if stripped.startswith("|") and stripped.endswith("|"):
            table_lines.append(line)

    _flush()
    return result


# ================================================================
# é€²æ—ãƒ»èª²é¡ŒæŠ½å‡º
# ================================================================

def extract_achievement_progress(computed_dir):
    """Read step1 tables and return per-channel achievement data."""
    files = [
        ("ç€é›»", computed_dir / "step1_ç€é›»ç€äºˆ.md"),
        ("SAL", computed_dir / "step1_SALç€äºˆ.md"),
        ("å•†è«‡", computed_dir / "step1_å•†è«‡å®Ÿæ–½ç€äºˆ.md"),
    ]

    progress = {}
    for label, path in files:
        if not path.exists():
            continue
        rows = parse_computed_table(path)
        for row in rows:
            ch = row.get("ãƒãƒ£ãƒãƒ«", "")
            rate = row.get("é”æˆç‡", "")
            mark = row.get("åˆ¤å®š", "")
            if ch not in progress:
                progress[ch] = {}
            progress[ch][label] = (rate, mark)

    return progress if progress else None


def extract_critical_issues(computed_dir):
    """Build critical issue descriptions with rate diagnosis and bad CVs."""
    issues_path = computed_dir / "step1_èª²é¡Œãƒãƒ£ãƒãƒ«.md"
    funnel_path = computed_dir / "step2_ãƒ•ã‚¡ãƒãƒ«è»¢æ›ç‡.md"
    cv_path = computed_dir / "step2_CVã‚³ãƒ³ãƒ†ãƒ³ãƒ„.md"

    if not issues_path.exists():
        return None

    issues_table = parse_computed_table(issues_path)
    funnel_by_ch = {}
    if funnel_path.exists():
        for row in parse_computed_table(funnel_path):
            funnel_by_ch[row.get("ãƒãƒ£ãƒãƒ«", "")] = row

    cv_by_ch = {}
    if cv_path.exists():
        cv_by_ch = parse_cv_tables(cv_path)

    result = []
    for row in issues_table:
        position = row.get("ä½ç½®ã¥ã‘", "")
        if "é‡ç‚¹èª²é¡Œ" not in position:
            continue

        ch = row.get("ãƒãƒ£ãƒãƒ«", "")
        funnel = funnel_by_ch.get(ch, {})

        # Determine which rate is the problem
        # å‰æœˆæ¯” = ãƒªãƒ¼ãƒ‰å‰æœˆæ¯”, å‰æœˆæ¯”_1 = CNç‡å‰æœˆæ¯”, å‰æœˆæ¯”_2 = SALç‡å‰æœˆæ¯”
        cn_rate = funnel.get("CNç‡", "")
        cn_mom = funnel.get("å‰æœˆæ¯”_1", "")
        sal_rate = funnel.get("SALç‡", "")
        sal_mom = funnel.get("å‰æœˆæ¯”_2", "")
        lead_mom = funnel.get("å‰æœˆæ¯”", "")

        # Build headline: what rate is the primary problem?
        headline_parts = []

        cn_is_bad = "ğŸ“‰" in cn_mom
        sal_is_bad = "ğŸ“‰" in sal_mom
        lead_drop = ""
        try:
            lv = float(lead_mom.replace("%", "").replace("+", ""))
            if lv <= -30:
                lead_drop = lead_mom
        except (ValueError, AttributeError):
            pass

        if cn_is_bad:
            headline_parts.append(f"CNç‡{cn_rate}ï¼ˆ{cn_mom.strip()}ï¼‰")
        if sal_is_bad:
            headline_parts.append(f"SALç‡{sal_rate}ï¼ˆ{sal_mom.strip()}ï¼‰")
        if lead_drop:
            desc = "åŠæ¸›" if float(lead_drop.replace("%", "").replace("+", "")) <= -40 else "å¤§å¹…æ¸›"
            headline_parts.append(f"ãƒªãƒ¼ãƒ‰{lead_drop}{desc}")

        # Fallback: show failing KPIs from step1
        if not headline_parts:
            bad_kpis = []
            for label, col in [("ç€é›»", "ç€é›»"), ("SAL", "SAL"), ("å•†è«‡", "å•†è«‡å®Ÿæ–½")]:
                val = row.get(col, "")
                if "âŒ" in val:
                    pct = re.search(r'(\d+%)', val)
                    bad_kpis.append(f"{label}{pct.group(1) if pct else ''}âŒ")
            headline_parts.append("ãƒ»".join(bad_kpis))

        headline = f"{ch}: {'ã€'.join(headline_parts)}"

        # Find bad CVs for this channel
        cv_detail = ""
        cv_rows = cv_by_ch.get(ch, [])
        if cv_rows:
            # Determine which rate to focus on
            focus_cn = cn_is_bad or (not sal_is_bad and not lead_drop)
            bad_cvs = []
            for cv in cv_rows:
                cv_name = cv.get("CVã‚³ãƒ³ãƒ†ãƒ³ãƒ„", "")
                cv_leads = cv.get("ãƒªãƒ¼ãƒ‰æ•°", "0")
                # Check for âš ï¸ marks on the problematic rate
                if focus_cn:
                    diff = cv.get("å·®åˆ†", "")
                    if "âš ï¸" in diff:
                        cv_cn = cv.get("CNç‡", "")
                        bad_cvs.append((cv_name, f"CN{cv_cn}", int(cv_leads.replace(",", "") or 0)))
                else:
                    # å·®åˆ†_1 = SALç‡ã®å·®åˆ† (due to duplicate header handling)
                    diff = cv.get("å·®åˆ†_1", "")
                    if "âš ï¸" in diff:
                        cv_sal = cv.get("SALç‡", "")
                        bad_cvs.append((cv_name, f"SAL{cv_sal}", int(cv_leads.replace(",", "") or 0)))

            # Sort by lead count desc, take top 3
            bad_cvs.sort(key=lambda x: x[2], reverse=True)
            if bad_cvs:
                cv_parts = [f"{name}({metric})" for name, metric, _ in bad_cvs[:3]]
                cv_detail = "  â†’ " + ", ".join(cv_parts)

        result.append(headline)
        if cv_detail:
            result.append(cv_detail)

    return result if result else None


# ================================================================
# ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ
# ================================================================

def find_latest_report():
    """reports/ é…ä¸‹ã®æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆã‚’è¿”ã™"""
    files = sorted(glob("reports/ãƒ¬ãƒãƒ¼ãƒˆ-*.md"))
    return files[-1] if files else None


def read_report(path):
    """ãƒ¬ãƒãƒ¼ãƒˆã‚’ (ã‚¿ã‚¤ãƒˆãƒ«, æœ¬æ–‡) ã«åˆ†é›¢"""
    with open(path, encoding="utf-8") as f:
        content = f.read()

    lines = content.split("\n")
    title_parts = []
    body_start = 0

    for i, line in enumerate(lines):
        if line.startswith("# "):
            title_parts.append(line[2:].strip())
            body_start = i + 1
        elif title_parts:
            break

    title = " / ".join(title_parts) if title_parts else "ãƒ¬ãƒãƒ¼ãƒˆ"
    body = "\n".join(lines[body_start:]).strip()
    return title, body


def extract_executive_summary(body):
    """ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
    lines = body.split("\n")
    out = []
    capturing = False

    for line in lines:
        if re.match(r"^##\s+.*ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒª", line):
            capturing = True
            continue
        if capturing and re.match(r"^##\s+", line):
            break
        if capturing and line.strip():
            out.append(line)

    return "\n".join(out).strip()


# ================================================================
# Markdown â†’ Notion ãƒ–ãƒ­ãƒƒã‚¯å¤‰æ›
# ================================================================

def _chunk(text):
    """rich_text ã® 2000æ–‡å­—åˆ¶é™ã‚’åˆ†å‰²"""
    while text:
        yield text[:MAX_RT_LEN]
        text = text[MAX_RT_LEN:]


def parse_rich_text(text):
    """Markdown inline â†’ Notion rich_text é…åˆ—ï¼ˆ**bold** å¯¾å¿œï¼‰"""
    if not text:
        return [{"type": "text", "text": {"content": ""}}]

    result = []
    for part in re.split(r"(\*\*[^*]+\*\*)", text):
        if not part:
            continue
        if part.startswith("**") and part.endswith("**"):
            for c in _chunk(part[2:-2]):
                result.append({
                    "type": "text",
                    "text": {"content": c},
                    "annotations": {"bold": True},
                })
        else:
            for c in _chunk(part):
                result.append({"type": "text", "text": {"content": c}})

    return result or [{"type": "text", "text": {"content": ""}}]


def _is_separator(line):
    return bool(re.match(r"^\s*\|[-:\s|]+\|\s*$", line))


def _is_table_row(line):
    s = line.strip()
    return s.startswith("|") and s.endswith("|")


def _parse_table(lines, start):
    """ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œç¾¤ â†’ Notion table ãƒ–ãƒ­ãƒƒã‚¯"""
    rows = []
    i = start
    while i < len(lines) and _is_table_row(lines[i]):
        if not _is_separator(lines[i]):
            cells = [c.strip() for c in lines[i].split("|")]
            if cells and cells[0] == "":
                cells = cells[1:]
            if cells and cells[-1] == "":
                cells = cells[:-1]
            rows.append(cells)
        i += 1

    if not rows:
        return None, i

    width = max(len(r) for r in rows)
    children = []
    for row in rows:
        while len(row) < width:
            row.append("")
        children.append({
            "type": "table_row",
            "table_row": {"cells": [parse_rich_text(c) for c in row[:width]]},
        })

    block = {
        "type": "table",
        "table": {
            "table_width": width,
            "has_column_header": True,
            "has_row_header": False,
            "children": children,
        },
    }
    return block, i


def markdown_to_blocks(body):
    """Markdown â†’ Notion API ãƒ–ãƒ­ãƒƒã‚¯é…åˆ—"""
    blocks = []
    lines = body.split("\n")
    i = 0

    while i < len(lines):
        line = lines[i]

        # ç©ºè¡Œ
        if not line.strip():
            i += 1
            continue

        # åŒºåˆ‡ã‚Šç·š
        if re.match(r"^---+\s*$", line):
            blocks.append({"type": "divider", "divider": {}})
            i += 1
            continue

        # ãƒ†ãƒ¼ãƒ–ãƒ«
        if _is_table_row(line):
            tbl, i = _parse_table(lines, i)
            if tbl:
                blocks.append(tbl)
            continue

        # è¦‹å‡ºã—ï¼ˆh4ä»¥ä¸Šã¯ h3 ã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
        m = re.match(r"^(#{1,6})\s+(.+)$", line)
        if m:
            lvl = min(len(m.group(1)), 3)
            ht = f"heading_{lvl}"
            blocks.append({
                "type": ht,
                ht: {"rich_text": parse_rich_text(m.group(2))},
            })
            i += 1
            continue

        # å¼•ç”¨ï¼ˆé€£ç¶šã™ã‚‹ > è¡Œã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰
        if line.startswith(">"):
            qlines = []
            while i < len(lines) and lines[i].startswith(">"):
                qlines.append(re.sub(r"^>\s?", "", lines[i]))
                i += 1
            blocks.append({
                "type": "quote",
                "quote": {"rich_text": parse_rich_text("\n".join(qlines))},
            })
            continue

        # ç•ªå·ä»˜ããƒªã‚¹ãƒˆ
        m = re.match(r"^(\d+)\.\s+(.+)$", line)
        if m:
            blocks.append({
                "type": "numbered_list_item",
                "numbered_list_item": {"rich_text": parse_rich_text(m.group(2))},
            })
            i += 1
            # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆå­é …ç›®
            while i < len(lines) and re.match(r"^\s+[-*]\s", lines[i]):
                sub = re.sub(r"^\s+[-*]\s+", "", lines[i])
                blocks.append({
                    "type": "bulleted_list_item",
                    "bulleted_list_item": {"rich_text": parse_rich_text(sub)},
                })
                i += 1
            continue

        # ç®‡æ¡æ›¸ã
        m = re.match(r"^[-*]\s+(.+)$", line)
        if m:
            blocks.append({
                "type": "bulleted_list_item",
                "bulleted_list_item": {"rich_text": parse_rich_text(m.group(1))},
            })
            i += 1
            continue

        # é€šå¸¸æ®µè½
        blocks.append({
            "type": "paragraph",
            "paragraph": {"rich_text": parse_rich_text(line)},
        })
        i += 1

    return blocks


# ================================================================
# Notion API
# ================================================================

def _notion_req(method, path, api_key, payload=None):
    data = json.dumps(payload).encode() if payload else None
    req = Request(f"{NOTION_API}{path}", data=data, method=method, headers={
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "Notion-Version": NOTION_VER,
    })
    try:
        with urlopen(req) as resp:
            return json.loads(resp.read())
    except HTTPError as e:
        body = e.read().decode("utf-8", errors="replace")
        print(f"  Notion API error: {e.code} {body}", file=sys.stderr)
        return None


def create_notion_page(api_key, database_id, title, blocks):
    """Notionãƒšãƒ¼ã‚¸ä½œæˆ â†’ URL ã‚’è¿”ã™"""
    now = datetime.now(JST)
    page_title = f"ãƒ¬ãƒãƒ¼ãƒˆ {now.strftime('%Y-%m-%d %H:%M')}"

    first = blocks[:MAX_BLOCKS]
    result = _notion_req("POST", "/pages", api_key, {
        "parent": {"database_id": database_id},
        "properties": {
            "ãƒšãƒ¼ã‚¸å": {"title": [{"text": {"content": page_title}}]},
        },
        "children": first,
    })

    if not result:
        print("  Notion page creation FAILED (see error above)", file=sys.stderr)
        return ""

    page_id = result["id"]
    page_url = result["url"]

    # æ®‹ã‚Šãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½è¨˜
    for s in range(MAX_BLOCKS, len(blocks), MAX_BLOCKS):
        chunk = blocks[s : s + MAX_BLOCKS]
        _notion_req("PATCH", f"/blocks/{page_id}/children", api_key, {
            "children": chunk,
        })

    return page_url


# ================================================================
# Slack
# ================================================================

def build_slack_message(mention_users, now, progress, issues, notion_url,
                        summary_fallback=None, period_start="", period_end=""):
    parts = []
    if mention_users:
        parts.append(" ".join(f"<@{u}>" for u in mention_users))
    title = (
        f"ãƒ‡ãƒ¢é›»è©±ãƒãƒ¼ãƒ  æœˆæ¬¡é€²æ—ãƒ¬ãƒãƒ¼ãƒˆ"
        f"ï¼ˆ{now.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')} å®Ÿè¡Œï¼‰"
    )
    if period_start and period_end:
        title += f"\nğŸ“… å‚ç…§æœŸé–“: {period_start} ã€œ {period_end}"
    parts.append(title)

    if progress:
        lines = ["ğŸ“Š *é”æˆé€²æ—ï¼ˆãƒãƒ£ãƒãƒ«åˆ¥ï¼‰*"]
        for ch in CHANNEL_ORDER:
            if ch not in progress:
                continue
            d = progress[ch]
            kpis = []
            for label in ["ç€é›»", "SAL", "å•†è«‡"]:
                if label in d:
                    rate, mark = d[label]
                    kpis.append(f"{label}{rate}{mark}")
            if kpis:
                lines.append(f"â€¢ {ch}: {' / '.join(kpis)}")
        parts.append("\n".join(lines))

    if issues:
        lines = ["ğŸš¨ *ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªèª²é¡Œ*"]
        for line in issues:
            if line.startswith("  â†’"):
                lines.append(line)
            else:
                lines.append(f"â€¢ {line}")
        parts.append("\n".join(lines))

    # Fallback: use executive summary if computed tables unavailable
    if not progress and not issues and summary_fallback:
        parts.append(summary_fallback)

    if notion_url:
        parts.append(f"ğŸ“ {notion_url}")
    return "\n\n".join(parts)


def send_slack_webhook(url, message):
    data = json.dumps({"text": message}).encode()
    req = Request(url, data=data, method="POST", headers={
        "Content-Type": "application/json",
    })
    try:
        with urlopen(req):
            print("  Slack webhook: sent")
            return True
    except (HTTPError, URLError) as e:
        print(f"  Slack webhook error: {e}", file=sys.stderr)
        return False


def send_slack_api(token, channel, message):
    data = json.dumps({"channel": channel, "text": message}).encode()
    req = Request("https://slack.com/api/chat.postMessage", data=data, method="POST",
                  headers={
                      "Authorization": f"Bearer {token}",
                      "Content-Type": "application/json",
                  })
    try:
        with urlopen(req) as resp:
            result = json.loads(resp.read())
            if result.get("ok"):
                print("  Slack API: sent")
                return True
            print(f"  Slack API error: {result.get('error')}", file=sys.stderr)
            return False
    except (HTTPError, URLError) as e:
        print(f"  Slack API error: {e}", file=sys.stderr)
        return False


# ================================================================
# ãƒ¡ã‚¤ãƒ³
# ================================================================

def main():
    notion_key = os.environ.get("NOTION_API_KEY", "")
    notion_db = os.environ.get("NOTION_DATABASE_ID", DEFAULT_DB_ID)
    slack_webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
    slack_token = os.environ.get("SLACK_BOT_TOKEN", "")
    slack_channel = os.environ.get("SLACK_CHANNEL", DEFAULT_CHANNEL)
    mention_env = os.environ.get("SLACK_MENTION_USERS", "")
    mention_users = mention_env.split(",") if mention_env else DEFAULT_MENTIONS

    if not notion_key and not slack_webhook and not slack_token:
        print("No credentials set. Skipping publish.")
        return

    report_path = find_latest_report()
    if not report_path:
        print("No report found in reports/. Skipping.")
        return

    print(f"Report: {report_path}")
    title, body = read_report(report_path)

    # --- Notion ---
    notion_url = ""
    if notion_key:
        print("Publishing to Notion...")
        blocks = markdown_to_blocks(body)
        print(f"  Blocks: {len(blocks)}")
        notion_url = create_notion_page(notion_key, notion_db, title, blocks)
        if notion_url:
            print(f"  URL: {notion_url}")
        else:
            print("  WARNING: Notion page creation failed, URL will not be in Slack message")
    else:
        print("Notion: skipped (NOTION_API_KEY not set)")

    # --- Slack ---
    now = datetime.now(JST)

    # Try structured format from computed tables
    computed_dir = Path("data/computed")
    progress = None
    issues = None
    period_start = ""
    period_end = ""
    if computed_dir.exists():
        try:
            progress = extract_achievement_progress(computed_dir)
            issues = extract_critical_issues(computed_dir)
        except Exception as e:
            print(f"  Warning: computed table parse failed: {e}", file=sys.stderr)
        # Extract period from any computed table's frontmatter
        try:
            fm_file = computed_dir / "step2_ãƒ•ã‚¡ãƒãƒ«è»¢æ›ç‡.md"
            if fm_file.exists():
                meta = parse_frontmatter(fm_file)
                period_start = meta.get("period_start", "")
                period_end = meta.get("period_end", "")
        except Exception:
            pass

    # Fallback to executive summary if computed tables unavailable
    summary_fallback = extract_executive_summary(body) if not progress else None
    message = build_slack_message(
        mention_users, now, progress, issues, notion_url,
        summary_fallback=summary_fallback,
        period_start=period_start, period_end=period_end,
    )

    if slack_webhook:
        print("Sending Slack notification (webhook)...")
        send_slack_webhook(slack_webhook, message)
    elif slack_token:
        print("Sending Slack notification (bot API)...")
        send_slack_api(slack_token, slack_channel, message)
    else:
        print("Slack: skipped (no credentials)")

    print("Done.")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"Publish error: {e}", file=sys.stderr)
        # ãƒ¬ãƒãƒ¼ãƒˆã¯æ—¢ã«ã‚³ãƒŸãƒƒãƒˆæ¸ˆã¿ãªã®ã§CIã‚’å¤±æ•—ã•ã›ãªã„
        sys.exit(0)
